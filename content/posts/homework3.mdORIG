---
title: "Master 2 - Human Computer Interaction: Homework 3 – Enhancing Human Abilities through Augmented Reality"
date: 2025-10-10
description: "Ideation, prototyping, and user testing for interfaces that extend human perception and cognition in the age of augmented reality."
tags: ["HCI", "Augmented Reality", "Prototyping", "Human Augmentation", "UX Research"]
---

# HCI – Homework 3  
*Enhancing Human Abilities in the Age of Augmented Reality*  
*Master’s in Computer Vision & AI — Gustavo*

> This exercise focuses on how **Augmented Reality (AR)** can be used to **extend human capabilities** — perceptual, cognitive, and physical.  
> Following the three-step design cycle (Ideate → Prototype → Evaluate), I developed and tested concepts that merge digital augmentation with everyday human experience.

---

## 1) Lecture 7 – Ideate (5 Points)

### Goal
To explore **three AR-based ideas** that enhance specific human abilities while remaining ethical, usable, and socially acceptable.

### **Idea 1 — AR Focus Lens**
> *Helping people maintain attention and manage distractions.*

**Problem:**  
In a hyper-connected environment, attention is fragmented. Notifications, multitasking, and external stimuli degrade deep focus.

**Concept:**  
An AR headset (or phone-based AR app) that **dynamically blurs or dims** irrelevant visual elements in the environment during focused tasks.  
The system detects what the user is concentrating on (e.g., a book, laptop, or whiteboard) and **softly masks unrelated objects** using depth and gaze tracking.

**Human ability enhanced:**  
- Sustained **attention** and **mental focus**  
- Reduction of **cognitive load**  

**Sketch:**  
![Idea 1 – AR Focus Lens](/assets/AR_focus_lens.png)

**Future potential:**  
Could integrate with EEG sensors or smartwatch data to detect distraction patterns in real time.

---

### **Idea 2 — AR Empathy Mirror**
> *Visualizing emotions to improve interpersonal understanding.*

**Problem:**  
Humans often misinterpret others’ emotions due to lack of nonverbal sensitivity, especially in cross-cultural or digital contexts.

**Concept:**  
An AR overlay (via smart glasses) that reads facial micro-expressions, tone of voice, and body posture, and **displays emotional cues** — e.g., “nervous,” “confident,” “tired” — as subtle color halos or icons around the person.

**Human ability enhanced:**  
- **Emotional intelligence** and **empathy**  
- Support for **social learning** and **therapy**

**Ethical considerations:**  
Requires **privacy safeguards** and user consent. The system should prioritize **self-awareness** training, not surveillance.

**Sketch:**  
<img src="/assets/AR_empathy_mirror.png"
     alt="Idea 2 – AR Empathy Mirror"
     style="height:700px; width:auto;" />

---

### **Idea 3 — AR Time Perception Assistant**
> *Making the invisible dimension of time perceptible.*

**Problem:**  
People struggle with abstract temporal awareness — how much time they spend on tasks, or how long until an event occurs.

**Concept:**  
An AR interface that **projects time visually in the environment**.  
Example: colored rings shrinking as deadlines approach, or subtle visual indicators floating above tasks representing urgency or duration.

**Human ability enhanced:**  
- **Temporal awareness** and **self-regulation**  
- Encourages mindful use of time rather than reactive multitasking

**Sketch:**  
<img src="/assets/AR_perception_assistant.png"
     alt="Idea 3 – AR Time Perception Assistant"
     style="height:600px; width:auto;" />

**Future integrations:**  
Could sync with calendar, to-do apps, or heart rate to modulate visualization intensity during stressful periods.

---

## 2) Lecture 8 – Prototype (5 Points)

> *“Build a prototype (paper, digital, or Wizard-of-Oz) that allows a first user exposure.”*

### Chosen Idea for Prototyping: **AR Focus Lens**

I created a **Wizard-of-Oz low-fidelity prototype** using a **smartphone camera overlay** and **semi-transparent filters** simulating dynamic focus.  
The prototype was built in **Figma** and tested on a tablet to simulate a see-through interface.

**Prototype features:**
1. **Central attention zone:** area remains sharp and colored.  
2. **Peripheral blur:** outer regions gradually fade into a soft blur.  
3. **Contextual prompt:** subtle animation that signals when focus drifts.  
4. **Manual override:** user can toggle blur level via gesture or tap.

**Technical inspiration:**  
Inspired by **foveated rendering** techniques from XR systems (eye-tracking-based rendering optimization).

**Prototype image (placeholder):**  
`![Prototype – AR Focus Lens Mockup](insert-prototype-image-here.jpg)`

**Goal of prototype:**  
Evaluate if users intuitively understand that blurred elements are *intentional guidance cues* and not visual bugs.

---

## 3) Lecture 9 – User Evaluation (5 Points)

> *Ask at least one person to interact with your prototype. Observe and reflect.*

### Participant
- **Profile:** 26-year-old engineering student, frequent multitasker, works with dual screens.  
- **Context:** tested the AR Focus Lens prototype during reading and note-taking activities.  
- **Device used:** iPad mockup (simulated AR overlay).

---

### **Observations**
- The user **immediately understood** that the blur indicated a “focus zone.”  
- Reported a feeling of **“peaceful isolation”** — helpful for studying.  
- Initially tried to **touch blurred areas**, expecting interactivity (interesting side effect).  
- Found the concept **less useful** when switching between multiple sources (e.g., laptop + book).  
- Requested **more adaptive focus regions** based on task recognition.

---

### **User Feedback (Quotes)**
> “It feels like the environment is helping me concentrate — almost like noise-cancelling for the eyes.”  
> “I’d like it to move automatically when I shift my gaze.”  
> “If it integrates with my schedule, it could tell when to take breaks too.”

---

### **Critical Reflection**
The test revealed strong **perceptual intuitiveness** but also highlighted:
- The risk of **over-automation**: users want control over when assistance activates.  
- Need for **transparent feedback** — the system must clearly indicate when and why it’s modifying the visual field.  
- Importance of **personal calibration**, since focus comfort zones vary between individuals.

From a cognitive standpoint, the prototype supports the **theory of attentional narrowing**, but it must avoid **overconstraining exploration**, which is vital for creativity.

---

## Final Insights

### Design Principles Learned
- Augmented Reality should **amplify human control**, not replace it.  
- Human augmentation requires **ethical framing** — especially when dealing with perception and emotion.  
- Simple prototypes can expose **deep behavioral insights** when paired with real observation.

### Next Steps
- Develop a **mid-fidelity prototype** using Unity AR Foundation or Apple VisionOS SDK.  
- Conduct **comparative tests** on cognitive workload (NASA-TLX).  
- Explore **multimodal feedback** (sound + vision) for immersive focus reinforcement.

---

> *Final reflection:*  
> Augmenting humans is not about adding layers of information — it’s about designing systems that **restore attention, empathy, and awareness** in a distracted world.  
> The future of HCI will belong to designs that act as **cognitive allies**, not distractions in disguise.
